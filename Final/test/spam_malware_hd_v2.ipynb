{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Spam / Phishing / URL Detection — HD Notebook"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This notebook loads your 3 datasets, cleans/merges them, trains models, evaluates with PR-AUC & F2, and writes final.csv."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, re, json, warnings, joblib\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, precision_recall_fscore_support, confusion_matrix\nfrom urllib.parse import urlparse\nfrom scipy.sparse import hstack\n\nBASE_DIR = \"spam_malware_hd_v2\"\nRAW_DIR  = os.path.join(BASE_DIR, \"data\", \"raw\")\nPROC_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\nMODEL_DIR= os.path.join(BASE_DIR, \"models\")\nAPP_DIR  = os.path.join(BASE_DIR, \"app\")\nfor d in [RAW_DIR, PROC_DIR, MODEL_DIR, APP_DIR]: os.makedirs(d, exist_ok=True)\nprint(BASE_DIR, RAW_DIR, PROC_DIR, MODEL_DIR, APP_DIR)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def normalise_label(series):\n    return series.map({\n        \"spam\":1,\"ham\":0,\"malicious\":1,\"benign\":0,\"phish\":1,\"legit\":0,\"legitimate\":0,\"defacement\":1\n    }).fillna(series).astype(int)\n\ndef load_sms(path):\n    df = pd.read_csv(path, encoding=\"latin-1\"); df.columns=[c.lower() for c in df.columns]\n    txt = \"text\" if \"text\" in df.columns else (\"message\" if \"message\" in df.columns else df.select_dtypes(\"object\").columns[0])\n    lab = \"label\" if \"label\" in df.columns else (\"category\" if \"category\" in df.columns else (\"spam\" if \"spam\" in df.columns else None))\n    if lab is None: raise ValueError(\"SMS dataset must contain a label/category/spam column\")\n    df[\"label\"] = normalise_label(df[lab])\n    out = df.rename(columns={txt:\"text\"})[[\"text\",\"label\"]].copy().drop_duplicates(subset=[\"text\"])\n    out[\"source\"] = \"sms\"; return out\n\ndef load_emails(path):\n    df = pd.read_csv(path); df.columns=[c.lower() for c in df.columns]\n    txt = \"text\" if \"text\" in df.columns else df.select_dtypes(\"object\").columns[0]\n    lab=None\n    for c in [\"label\",\"spam\",\"is_phish\",\"target\",\"class\"]:\n        if c in df.columns: lab=c; break\n    if lab is None: raise ValueError(\"Emails dataset missing a label-like column\")\n    df[\"label\"] = normalise_label(df[lab])\n    out = df.rename(columns={txt:\"text\"})[[\"text\",\"label\"]].dropna()\n    out[\"source\"]=\"phish\"; return out\n\ndef load_urls(path):\n    df = pd.read_csv(path); df.columns=[c.lower() for c in df.columns]\n    ucol = \"url\" if \"url\" in df.columns else [c for c in df.columns if \"url\" in c][0]\n    lab=None\n    for c in [\"label\",\"is_malicious\",\"spam\",\"target\",\"class\"]:\n        if c in df.columns: lab=c; break\n    if lab is None: raise ValueError(\"URLs dataset missing a label-like column\")\n    df[\"label\"] = normalise_label(df[lab])\n    out = df.rename(columns={ucol:\"text\"})[[\"text\",\"label\"]].dropna()\n    out[\"source\"]=\"url\"; return out"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sms_df   = load_sms(os.path.join(RAW_DIR, \"sms_spam.csv\"))\nemail_df = load_emails(os.path.join(RAW_DIR, \"emails.csv\"))\nurl_df   = load_urls(os.path.join(RAW_DIR, \"urls.csv\"))\nprint(\"Loaded:\", sms_df.shape, email_df.shape, url_df.shape)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def clean_text(s):\n    s = str(s).lower().strip()\n    s = re.sub(r\"[\\r\\n\\t]+\",\" \", s)\n    s = re.sub(r\"\\s+\",\" \", s)\n    return s\n\ndf = pd.concat([sms_df, email_df, url_df], ignore_index=True)\ndf[\"text\"] = df[\"text\"].map(clean_text)\nfinal_path = os.path.join(PROC_DIR, \"final.csv\")\ndf.to_csv(final_path, index=False)\nprint(\"Saved final:\", final_path, \"shape:\", df.shape)\ndf.head(3)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def evaluate_probabilities(y_true, prob_pos, beta=2.0):\n    precision, recall, thresholds = precision_recall_curve(y_true, prob_pos)\n    fbeta = (1+beta**2)*(precision*recall)/(beta**2*precision+recall+1e-12)\n    best_idx = int(np.nanargmax(fbeta)); best_thr = float(thresholds[max(0, best_idx-1)]) if best_idx>0 else 0.5\n    return {\"best_threshold\": best_thr, \"pr_auc\": float(average_precision_score(y_true, prob_pos)),\n            \"best_precision\": float(precision[best_idx]), \"best_recall\": float(recall[best_idx]), \"best_fbeta\": float(fbeta[best_idx])}\n\ndef report_at_threshold(y_true, prob_pos, thr):\n    y_pred = (prob_pos >= thr).astype(int)\n    p,r,f1,_ = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[1])\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    return {\"threshold\": float(thr), \"precision_malicious\": float(p[0]), \"recall_malicious\": float(r[0]),\n            \"f1_malicious\": float(f1[0]), \"confusion_matrix[[TN,FP],[FN,TP]]\": cm.tolist()}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Text models with TF-IDF word + char 3-5\ntext_df = df[df[\"source\"].isin([\"sms\",\"phish\"])].reset_index(drop=True)\nX_text = text_df[\"text\"].values; y_text = text_df[\"label\"].values\n\ntfidf_word = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\ntfidf_char = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\nXw = tfidf_word.fit_transform(X_text); Xc = tfidf_char.fit_transform(X_text); Xwc = hstack([Xw, Xc])\n\nlogreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\nsvm_cal = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"), method=\"sigmoid\", cv=3)\n\ndef cv_probs(estimator, X, y, cv=5):\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    out = np.zeros_like(y, dtype=float)\n    for tr, te in skf.split(X, y):\n        model = estimator\n        model.fit(X[tr], y[tr])\n        out[te] = model.predict_proba(X[te])[:,1]\n    return out\n\nprob_log = cv_probs(logreg, Xwc, y_text, cv=5)\nprob_svm = cv_probs(svm_cal, Xwc, y_text, cv=5)\n\nbest_log = evaluate_probabilities(y_text, prob_log, beta=2.0)\nbest_svm = evaluate_probabilities(y_text, prob_svm, beta=2.0)\nrep_log = report_at_threshold(y_text, prob_log, best_log[\"best_threshold\"])\nrep_svm = report_at_threshold(y_text, prob_svm, best_svm[\"best_threshold\"])\n\nbest_log, rep_log, best_svm, rep_svm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# URL features + RandomForest\ndef shannon_entropy(s):\n    if not s: return 0.0\n    p = np.array([s.count(c) for c in set(s)], dtype=float); p/=p.sum()\n    return float(-(p*np.log2(p+1e-12)).sum())\n\nimport re\nfrom urllib.parse import urlparse\n\ndef url_features(u):\n    try:\n        p = urlparse(u); host=p.netloc or \"\"; pathq=(p.path or \"\")+(\"?\" + p.query if p.query else \"\")\n        full = (p.netloc or \"\") + (p.path or \"\") + (p.query or \"\")\n    except:\n        host=\"\"; pathq=\"\"; full=str(u)\n    return {\"len\":len(u), \"dots\":u.count(\".\"), \"dashes\":u.count(\"-\"), \"digits\":sum(ch.isdigit() for ch in u),\n            \"specials\":sum(ch in \"!@#$%^&*()_+=[]{}|;:'\\\",<>?/\" for ch in u), \"entropy\":shannon_entropy(full),\n            \"num_subdomains\":host.count(\".\"), \"has_ip\":int(bool(re.search(r\"\\b\\d{1,3}(?:\\.\\d{1,3}){3}\\b\", host))),\n            \"tld_len\":len(host.split(\".\")[-1]) if \".\" in host else 0, \"path_len\":len(pathq)}\n\nurl_only = df[df[\"source\"]==\"url\"].reset_index(drop=True)\nfrom sklearn.ensemble import RandomForestClassifier\nX_url = pd.DataFrame([url_features(u) for u in url_only[\"text\"].tolist()])\ny_url = url_only[\"label\"].values\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nrf = RandomForestClassifier(n_estimators=400, class_weight=\"balanced\", random_state=42, n_jobs=-1)\nprob_rf = cross_val_predict(rf, X_url, y_url, cv=skf, method=\"predict_proba\")[:,1]\n\nbest_rf = evaluate_probabilities(y_url, prob_rf, beta=2.0)\nrep_rf  = report_at_threshold(y_url, prob_rf, best_rf[\"best_threshold\"])\nbest_rf, rep_rf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# K-Means clustering (themes)\ntfidf_clu = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\nXc = tfidf_clu.fit_transform(text_df[\"text\"].values)\nk = min(6, max(2, int(np.sqrt(len(text_df))//2)))\nkm = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\nlabs = km.fit_predict(Xc)\nterms = np.array(tfidf_clu.get_feature_names_out())\norder = km.cluster_centers_.argsort()[:, ::-1]\n{i: terms[order[i,:10]].tolist() for i in range(k)}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# IsolationForest anomaly detection (train on benign)\ntfidf_an = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\nXa = tfidf_an.fit_transform(text_df[\"text\"].values); ya = text_df[\"label\"].values\nbenign_mask = (ya==0)\niso = IsolationForest(n_estimators=400, random_state=42, contamination=\"auto\")\niso.fit(Xa[benign_mask].toarray())\nscores = iso.decision_function(Xa.toarray())\nprob_like = (scores.min() - scores); prob_like = (prob_like - prob_like.min())/(prob_like.max()-prob_like.min()+1e-12)\nbest_iso = evaluate_probabilities(ya, prob_like, beta=2.0); rep_iso = report_at_threshold(ya, prob_like, best_iso[\"best_threshold\"])\nbest_iso, rep_iso"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Fit final SVM on all text and save artifacts + threshold\nfrom scipy.sparse import hstack\ntfidf_word_f = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\ntfidf_char_f = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\nXw_f = tfidf_word_f.fit_transform(X_text); Xc_f = tfidf_char_f.fit_transform(X_text); Xwc_f = hstack([Xw_f, Xc_f])\nsvm_final = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"), method=\"sigmoid\", cv=3)\nsvm_final.fit(Xwc_f, y_text)\njoblib.dump(tfidf_word_f, os.path.join(MODEL_DIR, \"tfidf_word.joblib\"))\njoblib.dump(tfidf_char_f, os.path.join(MODEL_DIR, \"tfidf_char.joblib\"))\njoblib.dump(svm_final, os.path.join(MODEL_DIR, \"svm_calibrated.joblib\"))\n# Use previously found best_svm threshold from CV\nthr = 0.5\ntry:\n    thr = float(best_svm[\"best_threshold\"])\nexcept: pass\nwith open(os.path.join(MODEL_DIR, \"threshold.json\"), \"w\") as f:\n    json.dump({\"f2_threshold\": thr}, f, indent=2)\n\"Saved artifacts to \" + MODEL_DIR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Write Streamlit app\napp_py = \"\\n\".join([\n\"import os, json, joblib, numpy as np, pandas as pd, streamlit as st\",\n\"from scipy.sparse import hstack\",\n\"from sklearn.feature_extraction.text import TfidfVectorizer\",\n\"from sklearn.svm import LinearSVC\",\n\"from sklearn.calibration import CalibratedClassifierCV\",\n\"BASE_DIR = os.path.dirname(os.path.dirname(__file__))\",\n\"MODEL_DIR = os.path.join(BASE_DIR, 'models')\",\n\"st.set_page_config(page_title='Spam/Malware Detector', layout='centered')\",\n\"st.title('Spam / Phishing / URL Detector (HD Demo)')\",\n\"@st.cache_resource\",\n\"def load_artifacts():\",\n\"    tw = joblib.load(os.path.join(MODEL_DIR, 'tfidf_word.joblib'))\",\n\"    tc = joblib.load(os.path.join(MODEL_DIR, 'tfidf_char.joblib'))\",\n\"    clf = joblib.load(os.path.join(MODEL_DIR, 'svm_calibrated.joblib'))\",\n\"    thr = 0.5\",\n\"    p = os.path.join(MODEL_DIR, 'threshold.json')\",\n\"    if os.path.exists(p):\",\n\"        with open(p,'r') as f: thr = json.load(f).get('f2_threshold', 0.5)\",\n\"    return tw, tc, clf, float(thr)\",\n\"tw, tc, clf, F2_THR = load_artifacts()\",\n\"txt = st.text_area('Paste text or URL', height=160)\",\n\"if st.button('Analyze'):\",\n\"    if not txt.strip():\",\n\"        st.warning('Please paste something')\",\n\"    else:\",\n\"        Xw = tw.transform([txt.strip().lower()])\",\n\"        Xc = tc.transform([txt.strip().lower()])\",\n\"        X = hstack([Xw, Xc])\",\n\"        prob = clf.predict_proba(X)[:,1][0]\",\n\"        label = 'Malicious' if prob >= F2_THR else 'Benign'\",\n\"        st.subheader('Malicious probability: {:.3f} → Label: {} (thr={:.2f})'.format(prob, label, F2_THR))\",\n])\nwith open(os.path.join(APP_DIR, \"app.py\"), \"w\") as f:\n    f.write(app_py)\n\"App written to \" + os.path.join(APP_DIR, \"app.py\")"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}