{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# HD Spam / Phishing / URL Detection — Works With Your CSVs\n\nThis notebook inspects each CSV to map columns correctly and then trains/evaluates models per the HD rubric."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Setup & Paths"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, re, json, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_predict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (\n    precision_recall_curve, average_precision_score,\n    precision_recall_fscore_support, confusion_matrix\n)\nfrom urllib.parse import urlparse\nfrom scipy.sparse import hstack\nfrom joblib import dump\n\nDATA_DIR = \"/mnt/data\" if os.path.exists(\"/mnt/data/sms_spam.csv\") else \".\"\nSMS_PATH    = os.path.join(DATA_DIR, \"sms_spam.csv\")\nEMAILS_PATH = os.path.join(DATA_DIR, \"emails.csv\")\nURLS_PATH   = os.path.join(DATA_DIR, \"urls.csv\")\n\nBASE_DIR = os.path.abspath(\"hd_project\")\nRAW_DIR  = os.path.join(BASE_DIR, \"data\", \"raw\")\nPROC_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\nMODEL_DIR= os.path.join(BASE_DIR, \"models\")\nfor d in [RAW_DIR, PROC_DIR, MODEL_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nprint(\"Data dir:\", DATA_DIR)\nprint(\"Project dir:\", BASE_DIR)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Robust Loaders (auto-detect columns)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def _inspect(name, df):\n    print(f\"[{name}] columns:\", list(df.columns))\n    return df.head(3)\n\ndef _to_binary_labels(s):\n    mapping = {\n        \"spam\":1, \"ham\":0,\n        \"malicious\":1, \"benign\":0,\n        \"phish\":1, \"legit\":0, \"legitimate\":0,\n        \"defacement\":1, \"notspam\":0, \"not_spam\":0\n    }\n    if s.dtype.kind in \"iu\":\n        return s.astype(int)\n    return s.astype(str).str.strip().str.lower().map(lambda x: mapping.get(x, x)).astype(int)\n\ndef load_sms(path):\n    df = pd.read_csv(path, encoding=\"latin-1\")\n    df.columns = [c.strip().lower() for c in df.columns]\n    _inspect(\"SMS\", df)\n\n    if {\"v1\",\"v2\"}.issubset(df.columns):\n        df = df.rename(columns={\"v1\":\"label\",\"v2\":\"text\"})\n        df[\"label\"] = _to_binary_labels(df[\"label\"])\n        out = df[[\"text\",\"label\"]].copy()\n    else:\n        text_candidates  = [c for c in [\"text\",\"message\",\"sms\",\"msg\",\"content\"] if c in df.columns]\n        if not text_candidates:\n            str_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n            if str_cols: text_candidates=[str_cols[0]]\n        label_candidates = [c for c in [\"label\",\"category\",\"spam\",\"class\",\"target\"] if c in df.columns]\n        if not (text_candidates and label_candidates):\n            raise ValueError(\"SMS must have text + label-like column\")\n        tcol, lcol = text_candidates[0], label_candidates[0]\n        df[\"label\"] = _to_binary_labels(df[lcol])\n        out = df.rename(columns={tcol:\"text\"})[[\"text\",\"label\"]].copy()\n\n    out = out.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n    out[\"source\"] = \"sms\"\n    return out\n\ndef load_emails(path):\n    df = pd.read_csv(path)\n    df.columns = [c.strip().lower() for c in df.columns]\n    _inspect(\"EMAILS\", df)\n\n    if \"text\" in df.columns and \"spam\" in df.columns:\n        df[\"label\"] = _to_binary_labels(df[\"spam\"])\n        out = df.rename(columns={\"text\":\"text\"})[[\"text\",\"label\"]].dropna()\n    else:\n        text_candidates  = [c for c in [\"text\",\"email_text\",\"body\",\"message\",\"content\",\"subject_body\"] if c in df.columns]\n        if not text_candidates:\n            str_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n            if str_cols: text_candidates=[str_cols[0]]\n        label_candidates = [c for c in [\"label\",\"spam\",\"is_phish\",\"target\",\"class\"] if c in df.columns]\n        if not (text_candidates and label_candidates):\n            raise ValueError(\"Emails must have text + label-like column\")\n        tcol, lcol = text_candidates[0], label_candidates[0]\n        df[\"label\"] = _to_binary_labels(df[lcol])\n        out = df.rename(columns={tcol:\"text\"})[[\"text\",\"label\"]].dropna()\n\n    out[\"source\"] = \"phish\"\n    return out\n\ndef load_urls(path):\n    df = pd.read_csv(path)\n    df.columns = [c.strip().lower() for c in df.columns]\n    _inspect(\"URLS\", df)\n\n    url_col = \"url\" if \"url\" in df.columns else next((c for c in df.columns if \"url\" in c), None)\n    if not url_col: raise ValueError(\"URLs need a 'url' column.\")\n    label_col = next((c for c in [\"label\",\"is_malicious\",\"spam\",\"target\",\"class\",\"malicious\"] if c in df.columns), None)\n    if not label_col: raise ValueError(\"URLs need a label-like column.\")\n    df[\"label\"] = _to_binary_labels(df[label_col])\n    out = df.rename(columns={url_col:\"text\"})[[\"text\",\"label\"]].dropna()\n    out[\"source\"] = \"url\"\n    return out"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Load All Three & Preview"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "sms_df   = load_sms(SMS_PATH)\nemail_df = load_emails(EMAILS_PATH)\nurl_df   = load_urls(URLS_PATH)\n\nprint(\"Loaded shapes:\", sms_df.shape, email_df.shape, url_df.shape)\ndisplay(sms_df.head(3)); display(email_df.head(3)); display(url_df.head(3))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Clean Each & Save Clean Copies"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def clean_text(s):\n    s = str(s).lower().strip()\n    s = re.sub(r\"[\\r\\n\\t]+\",\" \", s)\n    s = re.sub(r\"\\s+\",\" \", s)\n    return s\n\nsms_clean    = sms_df.copy();   sms_clean[\"text\"]    = sms_clean[\"text\"].map(clean_text)\nemails_clean = email_df.copy(); emails_clean[\"text\"] = emails_clean[\"text\"].map(clean_text)\nurls_clean   = url_df.copy();   urls_clean[\"text\"]   = urls_clean[\"text\"].map(clean_text)\n\nsms_clean.to_csv(os.path.join(PROC_DIR, \"sms_clean.csv\"), index=False)\nemails_clean.to_csv(os.path.join(PROC_DIR, \"emails_clean.csv\"), index=False)\nurls_clean.to_csv(os.path.join(PROC_DIR, \"urls_clean.csv\"), index=False)\n\nprint(\"Saved cleaned copies to:\", PROC_DIR)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Merge → `data/processed/final.csv`"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "df = pd.concat([sms_clean, emails_clean, urls_clean], ignore_index=True)\nfinal_path = os.path.join(PROC_DIR, \"final.csv\")\ndf.to_csv(final_path, index=False)\nprint(\"Saved merged final:\", final_path, \"shape:\", df.shape)\ndisplay(df.source.value_counts())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Eval Helpers (PR‑AUC, F₂ threshold, Confusion Matrix)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def evaluate_probabilities(y_true, prob_pos, beta=2.0):\n    precision, recall, thresholds = precision_recall_curve(y_true, prob_pos)\n    fbeta = (1+beta**2)*(precision*recall)/(beta**2*precision + recall + 1e-12)\n    best_idx = int(np.nanargmax(fbeta))\n    best_thr = float(thresholds[max(0, best_idx-1)]) if best_idx>0 else 0.5\n    return {\"best_threshold\": best_thr,\n            \"best_precision\": float(precision[best_idx]),\n            \"best_recall\": float(recall[best_idx]),\n            \"best_fbeta\": float(fbeta[best_idx]),\n            \"pr_auc\": float(average_precision_score(y_true, prob_pos))}\n\ndef report_at_threshold(y_true, prob_pos, thr):\n    y_pred = (prob_pos >= thr).astype(int)\n    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[1])\n    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n    return {\"threshold\": float(thr),\n            \"precision_malicious\": float(p[0]),\n            \"recall_malicious\": float(r[0]),\n            \"f1_malicious\": float(f1[0]),\n            \"confusion_matrix[[TN,FP],[FN,TP]]\": cm.tolist()}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Text Models: TF‑IDF (word 1–2 + char 3–5) → LogReg & Calibrated Linear SVM (5‑fold CV)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "text_df = df[df[\"source\"].isin([\"sms\",\"phish\"])].reset_index(drop=True)\nX_text = text_df[\"text\"].values\ny_text = text_df[\"label\"].values\n\ntfidf_word = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\ntfidf_char = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\nXw = tfidf_word.fit_transform(X_text)\nXc = tfidf_char.fit_transform(X_text)\nXwc = hstack([Xw, Xc])\n\nlogreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\nsvm_cal = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"), method=\"sigmoid\", cv=3)\n\ndef cv_probs(estimator, X, y, cv=5):\n    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    out = np.zeros_like(y, dtype=float)\n    for tr, te in skf.split(X, y):\n        est = estimator\n        est.fit(X[tr], y[tr])\n        out[te] = est.predict_proba(X[te])[:,1]\n    return out\n\nprob_log = cv_probs(logreg, Xwc, y_text, cv=5)\nprob_svm = cv_probs(svm_cal, Xwc, y_text, cv=5)\n\nbest_log = evaluate_probabilities(y_text, prob_log, beta=2.0)\nbest_svm = evaluate_probabilities(y_text, prob_svm, beta=2.0)\nrep_log  = report_at_threshold(y_text, prob_log, best_log[\"best_threshold\"])\nrep_svm  = report_at_threshold(y_text, prob_svm, best_svm[\"best_threshold\"])\n\nprint(\"=== Text: Logistic Regression ===\")\nprint(json.dumps(best_log, indent=2)); print(json.dumps(rep_log, indent=2))\nprint(\"=== Text: Linear SVM (Calibrated) ===\")\nprint(json.dumps(best_svm, indent=2)); print(json.dumps(rep_svm, indent=2))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) URL Model: RandomForest (5‑fold CV) on engineered URL features"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def shannon_entropy(s):\n    if not s: return 0.0\n    p = np.array([s.count(c) for c in set(s)], dtype=float); p/=p.sum()\n    return float(-(p*np.log2(p + 1e-12)).sum())\n\nimport re\ndef url_features(u):\n    try:\n        p = urlparse(u); host=p.netloc or \"\"; pathq=(p.path or \"\")+(\"?\" + p.query if p.query else \"\")\n        full = (p.netloc or \"\") + (p.path or \"\") + (p.query or \"\")\n    except:\n        host=\"\"; pathq=\"\"; full=str(u)\n    return {\"len\": len(u), \"dots\": u.count(\".\"), \"dashes\": u.count(\"-\"),\n            \"digits\": sum(ch.isdigit() for ch in u),\n            \"specials\": sum(ch in \"!@#$%^&*()_+=[]{}|;:'\\\\\\\",<>?/\" for ch in u),\n            \"entropy\": shannon_entropy(full), \"num_subdomains\": host.count(\".\"),\n            \"has_ip\": int(bool(re.search(r\"\\\\b\\\\d{1,3}(?:\\\\.\\\\d{1,3}){3}\\\\b\", host))),\n            \"tld_len\": len(host.split(\".\")[-1]) if \".\" in host else 0,\n            \"path_len\": len(pathq)}\n\nurl_only = df[df[\"source\"]==\"url\"].reset_index(drop=True)\nX_url = pd.DataFrame([url_features(u) for u in url_only[\"text\"].tolist()])\ny_url = url_only[\"label\"].values\n\nrf = RandomForestClassifier(n_estimators=400, class_weight=\"balanced\", random_state=42, n_jobs=-1)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nprob_rf = cross_val_predict(rf, X_url, y_url, cv=skf, method=\"predict_proba\")[:,1]\n\nbest_rf = evaluate_probabilities(y_url, prob_rf, beta=2.0)\nrep_rf  = report_at_threshold(y_url, prob_rf, best_rf[\"best_threshold\"])\n\nprint(\"=== URL: RandomForest ===\")\nprint(json.dumps(best_rf, indent=2)); print(json.dumps(rep_rf, indent=2))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Extra 1 — K‑Means (themes)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if len(text_df) >= 6:\n    tfidf_clu = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\n    Xc2 = tfidf_clu.fit_transform(text_df[\"text\"].values)\n    k = min(6, max(2, int(np.sqrt(len(text_df))//2)))\n    km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\")\n    labels = km.fit_predict(Xc2)\n    terms = np.array(tfidf_clu.get_feature_names_out())\n    order = km.cluster_centers_.argsort()[:, ::-1]\n    cluster_top_terms = {i: terms[order[i,:10]].tolist() for i in range(k)}\n    print(\"Top terms per cluster:\")\n    for i in range(k):\n        print(i, \":\", cluster_top_terms[i])\nelse:\n    print(\"[K-Means skipped] Not enough text rows.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Extra 2 — IsolationForest (benign-only training)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if len(text_df) >= 10:\n    tfidf_an = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\n    Xa = tfidf_an.fit_transform(text_df[\"text\"].values)\n    ya = text_df[\"label\"].values\n    benign_mask = (ya==0)\n    iso = IsolationForest(n_estimators=400, random_state=42, contamination=\"auto\")\n    iso.fit(Xa[benign_mask].toarray())\n    scores = iso.decision_function(Xa.toarray())\n    prob_like = (scores.min() - scores)\n    prob_like = (prob_like - prob_like.min())/(prob_like.max()-prob_like.min()+1e-12)\n    best_iso = evaluate_probabilities(ya, prob_like, beta=2.0)\n    rep_iso  = report_at_threshold(ya, prob_like, best_iso[\"best_threshold\"])\n    print(\"=== IsolationForest (text) ===\")\n    print(json.dumps(best_iso, indent=2)); print(json.dumps(rep_iso, indent=2))\nelse:\n    print(\"[IsolationForest skipped] Not enough text rows.\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Save Final Text Model (+ threshold)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "tfidf_word_f = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=1)\ntfidf_char_f = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), min_df=1)\nXw_f = tfidf_word_f.fit_transform(X_text)\nXc_f = tfidf_char_f.fit_transform(X_text)\nXwc_f = hstack([Xw_f, Xc_f])\n\nsvm_final = CalibratedClassifierCV(LinearSVC(class_weight=\"balanced\"), method=\"sigmoid\", cv=3)\nsvm_final.fit(Xwc_f, y_text)\n\ndump(tfidf_word_f, os.path.join(MODEL_DIR, \"tfidf_word.joblib\"))\ndump(tfidf_char_f, os.path.join(MODEL_DIR, \"tfidf_char.joblib\"))\ndump(svm_final,    os.path.join(MODEL_DIR, \"svm_calibrated.joblib\"))\n\nthr = 0.5\ntry: thr = float(best_svm[\"best_threshold\"])\nexcept: pass\nwith open(os.path.join(MODEL_DIR, \"threshold.json\"), \"w\") as f:\n    json.dump({\"f2_threshold\": thr}, f, indent=2)\n\nprint(\"Artifacts saved to:\", MODEL_DIR)"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}